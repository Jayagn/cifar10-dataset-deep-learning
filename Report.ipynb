{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data has been scaled by using MinMaxScaler.\n",
    "- Other scaling functions were also used such as Robust Scaler and MinMaxScaler, but Standard Scaling performed best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For output optimizer used is 'adam'(mentioned).\n",
    "- 'categorical crossentropy' is used as loss function because it is multi-class problem and hence we cannot use binary crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in Layers and Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following is the table of change in number of layer and its corresponding obtained accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layers | Neurons per Layer | Training accuracy | Validation Accuracy |\n",
    "| --- | --- | --- | --- | \n",
    "| 2 | 512 | 31 | 41 |\n",
    "| 2 | 1024 | 35 | 44 |\n",
    "| 3 | 512 | 40 | 46 | \n",
    "| 3 | 1024 | 45 | 45 | \n",
    "| 4 | 512 | 50 | 50 | \n",
    "| 4 | 1024 | 55 | 58 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP vs CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the results we can clearly see that CNN performs much better than MLP.\n",
    "- The main disadvantage of MLP is that as the number of total parameters grows to very high. This is inefficient because there is redundancy in such high dimensions.\n",
    "- Another reason why CNN outperforms MLP is because of convolution and pooling layer that CNN has.\n",
    "- Convolutional layers take advantage of the local spatial coherence of the input. This is only possible because we assume that spatially close inputs are correlated. For images, this can be seen by the fact that the image loses its meaning when the pixels are shuffled.\n",
    "- Using this property, CNNs are able to cut down on the number of parameter by sharing weights. This makes them extremely efficient in image processing, compared to multi-layer perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below are graphs of Loss and Accuracies of CNN1 and CNN2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss | Accuracy\n",
    "- | - \n",
    "![alt text](CNN1.png \"Title\") | ![alt text](CNN1_.png \"Title\")\n",
    "![alt text](CNN2.png \"Title\") | ![alt text](CNN_.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From figure it is clearly visible that as number of epochs increases , training loss decreases and thus training accuracy increases.\n",
    "- In case of CNN1, although training loss decreases , the validation loss keeps on increases.\n",
    "- The reason for this is CNN1 does have any 'dropout', this causes the model to be over fitting, that is just memorizing the training data.\n",
    "- In general a overfitting model can be improved by adding dropout, which is added in CNN2. Hence, it is clearly visible that in CNN2 training loss decreases and also the validation loss decreases.\n",
    "- Hence, for CNN2 the training as well as validation accuracy both increases with number of epochs due to dropout.\n",
    "- Increasing the epochs might increase the accuracy, but not necessarily it means that MORE epochs means MORE accuracy, it may cause model to overfit. We can add early stopping to check if number of epochs are appropriate without overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can improve model by following ways\n",
    "    - add noise to dense or convolution layers\n",
    "    - add drop-out layers\n",
    "    - add l1 or l2 regularizers\n",
    "    - add early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Volume, high price, close price and open price are considered of last 3 days to predict the open price of present day.\n",
    "- Dataset is created by concatenting the feature mentioned above by appending the open price of present day to the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard scaler is used and then scaler objects are saved to use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As this dataset is timeseries dataset, LSTM model is used.\n",
    "- 3 layers is used in the model.\n",
    "- Also, I have used 'rmsprop' optimizer as it converges faster then 'adam'.\n",
    "- dropout is added to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](plot.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increase in number of days would increase the model's robustness, as we get more data to predict the output.\n",
    "- It may also reduce the overfitting of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First the data is downloaded from and is unzipped using python's url and tar library.\n",
    "- Train data which contains 12,500 files of positive reviews and 12,500 files of negative reviews.\n",
    "- By using 're'  module of python special characters are removed and replaced by space.\n",
    "- Data is then splitted by which I obtained tokenized words of sentences and stored as a list.\n",
    "- Word Embeddings are created using Gensim Word2vec library. (Specific parameters are kept for that process)\n",
    "- Embeddings are saved as w2v_embeddings.txt.\n",
    "- I used tokenizer function from Keras to count the unique words in our vocabulary and assign each of those words to indices.\n",
    "- word sequences are generated using text_to_sequences function and then data is padded to get uniform length using pad_sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The embedding matrix which was generated before is fed as layer 1 of neural network.\n",
    "- The layer 2 is MaxPooling1D layer with pool_length equal to 5.\n",
    "- The layer 3 and 4 are Dense layer with activation function reLu.\n",
    "- Finally model is compiled using adam optimizer and loss as binary crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters vs accuracy table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Activation function | batch size | epochs | Training accuracy | Validation Accuracy | Testing Accuracy |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| sigmoid | 64 | 15 | 53 | 52 | 52 |\n",
    "| sigmoid | 128 | 20 | 58 | 56 | 55 |\n",
    "| sigmoid | 128 | 25 | 62 | 62 | 62 |\n",
    "| reLu | 128 | 15 | 58 | 56 | 56 |\n",
    "| reLu | 128 | 25 | 60 | 60 | 60 |\n",
    "| reLu | 128 | 50 | 72 | 70 | 70 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
